{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"jumbotron\">\n",
    "  <h1 class=\"display-4\" style=\"color:Blue;\">Deep analysis of Linear Regression</h1> \n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "[![Build Status](https://github.com/mattcone/markdown-guide/workflows/tests/badge.svg?branch=master)](https://github.com/santoshkumarbvp)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Programming languages ( e.g. Python and R ) have really made the life of the developer pretty easy going. With just a few lines of code in any of the programming language is sufficient enough to do huge amount of mathematical calculations, and provide the end solution.  \n",
    "\n",
    "Moreover, knowing the intuition behind the algorithm would empower the developer even more. \n",
    "\n",
    "> This blog is concentrating the very basic algorithm of machine learning, **``Linear Regression``**. \n",
    "\n",
    "Learnings from this article would be useful to understand the rest of the algorithms in machine learning and deep learning. Topics like below would be regularly used in machine learning field:\n",
    "><ul>\n",
    ">    <li> Loss Function\n",
    ">    <li> Gradient Descent\n",
    ">    <li> Weight initialization\n",
    "></ul>\n",
    "\n",
    "<br>\n",
    "\n",
    "```Note : For theoratical background about the topic. Luis Serrano lecture is very informative``` [video](https://www.youtube.com/watch?v=wYPUhge9w5c)\n",
    "\n",
    "<br>\n",
    "\n",
    "### Background\n",
    "\n",
    "><ol>\n",
    "> <li> What is Linear Regression : It is a model which showcase linear behavior between dependent and independent variables. \n",
    " $$ f(x) = y = m x + c $$\n",
    "><li> Above equation highlights the relation between two variables, and which later could be used to predict the dependent values(y).\n",
    "> <li>Equation(model) has its weight and bias which would be determined through training process.\n",
    "></ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Synopsis\n",
    "\n",
    "First, Lets discuss the whole algorithm at high level before going into deep inside.\n",
    "><ol>\n",
    ">  <li> Will take the fake linear data for the model\n",
    "> <br>\n",
    "> <br>\n",
    ">   <li> will intialize the weight($m$) and bias($c$) for the first iteration. Initially, we have freedom to select regression line, so $ y = y_{mean} $ equation is considered as regression line. thus, $m$ would be $zero$ and $c$ would be $y_{mean}$ for first iteration.\n",
    "> <br>\n",
    "> <br>\n",
    ">   <li> will calculate the $loss$ for $m$ and $c$\n",
    ">          $$Loss ( L) = \\sum_{j=0}^{n} \\frac{1}{2n} (\\hat{y_i}-y_i)^2$$\n",
    ">       where,         $$\\hat{y_i} = mx_i +c $$ \n",
    "> <br>\n",
    "> Note: Numeric 2 is used just to avoid the scalar number in gradient later on. \n",
    "> <br>\n",
    "> <br>\n",
    "> <li> will calculate the $gradient$ with respect to weight ($m$) and bias ($c$)\n",
    "> <br>\n",
    "> <br>\n",
    "> $$ \\frac{\\partial L}{\\partial m} = \\frac{\\partial (\\sum_{j=0}^{n} \\frac{1}{2n} (\\hat{y_i}-y_i)^2)}{\\partial m}    $$\n",
    "> <br>\n",
    "> <br>\n",
    "> $$ \\frac{\\partial L}{\\partial m} = \\frac{\\partial (\\sum_{j=0}^{n} \\frac{1}{2n} (mx_i +c -y_i)^2)}{\\partial m}$$ \n",
    ">  Now applying chain rule,\n",
    "> <br>\n",
    "> <br>\n",
    ">  $$ \\frac{\\partial L}{\\partial m} =\\frac{1}{2n} (2) \\sum_{j=0}^{n}(\\hat{y_i}-y_i) \\frac{\\partial (\\sum_{j=0}^{n} (mx_i +c -y_i)}{\\partial m} $$    \n",
    "> <br>\n",
    "> <br>\n",
    ">      $c$ and $y_i$ both are constant wrt $m$ so their derivative would be zero. \n",
    "> <br>\n",
    "> <br>\n",
    "> <p style=\"background-color:powderblue;\"> $$ \\frac{\\partial L}{\\partial m} =\\frac{1}{n} \\sum_{j=0}^{n} \\begin{Bmatrix}(\\hat{y_i}-y_i)  (x_i) \\end{Bmatrix} $$</p>  \n",
    "> <br>    \n",
    "> <br>   \n",
    "> <li> Similarly, $gradient$ wrt $c$ would be derived\n",
    ">  $$ \\frac{\\partial L}{\\partial c} = \\frac{\\partial (\\sum_{j=0}^{n} \\frac{1}{2n} (\\hat{y_i}-y_i)^2)}{\\partial c}    $$ \n",
    ">   Now applying chain rule again,\n",
    "> <br>    \n",
    "> <br> \n",
    ">  $$ \\frac{\\partial L}{\\partial c} =\\frac{1}{2n} (2) \\sum_{j=0}^{n}(\\hat{y_i}-y_i) \\frac{\\partial (\\sum_{j=0}^{n} (mx_i +c -y_i)}{\\partial c} $$   \n",
    "> <p style=\"background-color:powderblue;\"> $$ \\frac{\\partial L}{\\partial c} =\\frac{1}{n} \\sum_{j=0}^{n} \\begin{Bmatrix}(\\hat{y_i}-y_i)\\end{Bmatrix}\t$$</p> \n",
    "> <br>    \n",
    "> <br>\n",
    "> <li> Will update the $weight$ and $bias$ with gradient.\n",
    "> <br>In below equation, $lr$ is learning rate which is a hyperparameter.\n",
    "> <br>    \n",
    "> <br>\n",
    ">  $$  m_{new} = m_{old} - (lr) (\\frac{\\partial L}{\\partial m}) $$\n",
    ">  or from step $4$, placing derivative value, \n",
    "> <p style=\"background-color:powderblue;\">$$ m_{new} = m_{old} - (lr) (\\frac{1}{n} \\sum_{j=0}^{n} \\begin{Bmatrix}(\\hat{y_i}-y_i) (x_i)\\end{Bmatrix}) $$ </p>\n",
    "> \n",
    "> <br>    \n",
    "> <br>\n",
    "> Similarly, updating value for $c$\n",
    ">\n",
    "> $$  c_{new} = c_{old} - (lr) (\\frac{\\partial L}{\\partial c}) $$\n",
    ">\n",
    "> or from step $5$, placing derivative value,\n",
    "> <p style=\"background-color:powderblue;\"> $$ c_{new} = c_{old} - (lr) (\\frac{1}{n} \\sum_{j=0}^{n} \\begin{Bmatrix}(\\hat{y_i}-y_i)\\end{Bmatrix}) $$ </p>\n",
    ">\n",
    "> <br>    \n",
    "> <br>\n",
    "> <li> Repeat all the steps from $3$ till the $gradient$ is very small, which leads to weight and bias not changing. That is the point where $weight$ and $bias$ values will decide the model.  $$ y = m_{final} x + c_{final}$$\n",
    "></ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "<h1 style=\"font-size:150%; background-color:lightyellow;\">Step 1</h1>\n",
    "\n",
    "\n",
    "Fake data prepration \n",
    "- Weight and bias are known, where $m = 5$ and $c = 11$\n",
    "- Will predict weight and bias from algorithm and later will compare with known values\n",
    "- Adding a noise in equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "x = np.linspace(1,20,20)\n",
    "e = np.random.randint(-9,8,20)\n",
    "\n",
    "\n",
    "w_actual = [3,11]\n",
    "\n",
    "y =  w_actual[0]*x + w_actual[1] + e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.scatterplot(x=x,y=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------\n",
    "<!-- **```Step 2```**-->\n",
    "<h1 style=\"font-size:150%; background-color:lightyellow;\">Step 2</h1>\n",
    "\n",
    "- Initializing the weight and bias with random regression line. \n",
    "- $y=y_{mean}$ is selected for first regression line, which is horizontal starting line.\n",
    "- $m = 0$ and $ c = y_{mean} $\n",
    "- One list for weight and bias value, with name $w$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 0\n",
    "c = y.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = [m,c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 42.7]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------\n",
    "<h1 style=\"font-size:150%; background-color:lightyellow;\">Step 3</h1>\n",
    "\n",
    "- calculating the <br> $ Loss ( L) = \\sum_{j=0}^{n} \\frac{1}{2n} (\\hat{y_i}-y_i)^2$ <br><br>\n",
    "- Before that $\\hat{y}$ is to be calculated. \n",
    "<br>\n",
    "$\\hat{y_i} = mx_i +c $ <br> <br>\n",
    "- $w$ contains both starting value of $weight$ and $bias$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def y_hat(x,w):\n",
    "    return w[0]*x + w[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(x,y,w):\n",
    "    y_hat_v = y_hat(x,w)\n",
    "    n=len(y)\n",
    "    return (0.5/n)*sum([(y_hat_v[i]-y_v)**2 for i,y_v in enumerate(y)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:150%; background-color:lightyellow;\">Step 4, 5 </h1>\n",
    "\n",
    "\n",
    "- Calculating the gradient for $m$ and $c$ respectively <br><br> $ \\frac{\\partial L}{\\partial m} =\\frac{1}{n} \\sum_{j=0}^{n} \\begin{Bmatrix}(\\hat{y_i}-y_i)  (x_i) \\end{Bmatrix} $ <br><br>\n",
    " $ \\frac{\\partial L}{\\partial c} =\\frac{1}{n} \\sum_{j=0}^{n} \\begin{Bmatrix}(\\hat{y_i}-y_i)\\end{Bmatrix}\t$\n",
    " <br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(x ,y,w):\n",
    "    g = [0]*2\n",
    "    g[0] = (1/len(y)) * sum((y_hat(x,w) - y)* x)\n",
    "    g[1] = (1/len(y)) * sum(y_hat(x,w) - y) \n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:150%; background-color:lightyellow;\">Step 6</h1>\n",
    "\n",
    "\n",
    "- Updating the $weight$ and $bias$ <br><br>\n",
    " $ m_{new} = m_{old} - (lr) (\\frac{1}{n} \\sum_{j=0}^{n} \\begin{Bmatrix}(\\hat{y_i}-y_i) (x_i)\\end{Bmatrix}) $\n",
    " <br><br>\n",
    " $ c_{new} = c_{old} - (lr) (\\frac{1}{n} \\sum_{j=0}^{n} \\begin{Bmatrix}(\\hat{y_i}-y_i)\\end{Bmatrix}) $ \n",
    " <br><br>\n",
    " - Now updated $weight$ and $bias$ would be used to calculate $loss$, then the gradient to update the $weight$ and $bias$.\n",
    " - This process of updating the weight and bias would go on, till gradient becomes minisculy small.\n",
    " - Here we are stopping the updation after 4000 iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def descent(w_new,w_old,lr):\n",
    "    \n",
    "    print(f'Initial weight and bias {w_old}' )\n",
    "    j=0\n",
    "    while True:\n",
    "        w_old = w_new\n",
    "        w0 = w_old[0] - lr*grad(x,y,w_old)[0]\n",
    "        w1 = w_old[1] - lr*grad(x,y,w_old)[1] \n",
    "        w_new= [w0,w1]\n",
    "        if j%300 == 0:        \n",
    "            print(f'loss is {loss(x,y,w_new)}. Weight and bias are {w_new[0],w_new[1]}')\n",
    "        if j > 4000:\n",
    "            return w_new\n",
    "        j+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:150%; background-color:lightyellow;\">Step 7 </h1>\n",
    "\n",
    "- Envoking the code with initial $weight$ and $bias$\n",
    "- Learning rate is hyperparameter, it should be optimum, between 0.01 to 0.001. All depends on data. \n",
    "- Highlighting the model with final $weight$ and $bias$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 42.7]\n",
      "loss is 127.52229293750003. Weight and bias are (0.9754999999999998, 42.7)\n",
      "loss is 38.8051475079198. Weight and bias are (1.8047441620777256, 27.300856161223194)\n",
      "loss is 18.194567763146278. Weight and bias are (2.3687612758828154, 19.60500225473718)\n",
      "loss is 13.03227833383949. Weight and bias are (2.651033634977644, 15.753475795233921)\n",
      "loss is 11.739290340200405. Weight and bias are (2.7923018390068144, 13.825911128790384)\n",
      "loss is 11.415438302342222. Weight and bias are (2.8630020150127926, 12.86122723465436)\n",
      "loss is 11.334323749894041. Weight and bias are (2.8983851709156183, 12.37843408545097)\n",
      "loss is 11.314007155123218. Weight and bias are (2.9160932982834744, 12.136811698745232)\n",
      "loss is 11.308918499504422. Weight and bias are (2.9249556444161198, 12.015887493723469)\n",
      "loss is 11.307643954419538. Weight and bias are (2.92939096258067, 11.955368832520293)\n",
      "loss is 11.30732472173903. Weight and bias are (2.9316106961508037, 11.925081196118937)\n",
      "loss is 11.307244764187496. Weight and bias are (2.9327216011500803, 11.909923211779168)\n",
      "loss is 11.307224737384352. Weight and bias are (2.933277573242417, 11.902337129913956)\n",
      "loss is 11.307219721312245. Weight and bias are (2.9335558193279585, 11.89854054081769)\n"
     ]
    }
   ],
   "source": [
    "w = descent(w,w,0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final weight is 2.93 and bias is 11.9 from the algorithm\n",
      "Actual weight is 3 and bias is 11\n"
     ]
    }
   ],
   "source": [
    "print(f'Final weight is {w[0].round(2)} and bias is {w[1].round(2)} from the algorithm')\n",
    "print(f'Actual weight is {w_actual[0]} and bias is {w_actual[1]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"jumbotron\">\n",
    "  <h1 class=\"display-4\" style=\"color:Blue;\">Nice!! We are able to explore the entire linear regression algorithm mechanics.This is an amazing feeling to know, what is happening in the backhand.  </h1> \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
